{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nyjyu4FzUAVw"},"source":["# 신경망 학습"]},{"cell_type":"markdown","metadata":{"id":"VQvNez4qydhL"},"source":["## 단순한 신경망 구현 : Logic Gate"]},{"cell_type":"markdown","metadata":{"id":"-7te43hqyiiJ"},"source":["### 필요한 모듈 import"]},{"cell_type":"code","metadata":{"id":"Qf2F_YbdybBE"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-whitegrid')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orUoPmDcymhj"},"source":["### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","metadata":{"id":"bOAmMxo0ymDF"},"source":["epochs = 1000\n","lr = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjmLWgFVysnq"},"source":["### 유틸 함수들(Util Functions)"]},{"cell_type":"code","metadata":{"id":"Y4OMFGrjyq1c"},"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def mean_squared_error(y_pred, y_true):\n","    return np.mean(np.power(y_true - y_pred, 2))\n","\n","def cross_entropy_error(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    return -np.sum(y_true * np.log(y_pred + delta))\n","\n","def cross_entropy_error_for_batch(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    batch_size = y_pred.shape[0]\n","    return -np.sum(y_true * np.log(y_pred + delta)) / batch_size\n","\n","def cross_entropy_error_for_bin(y_pred, y_true):\n","    return 0.5 * np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","    return y\n","\n","def differential(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","      temp_val = x[i]\n","\n","      x[i] = temp_val + eps\n","      f_h1 = f(x)\n","      x[i] = temp_val - eps\n","      f_h2 = f(x)\n","\n","      diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","      x[i] = temp_val\n","\n","    return diff_value\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5Z2LTT_y3i5"},"source":["### 신경망"]},{"cell_type":"code","metadata":{"id":"gMTjjYgdy3D8"},"source":["class LogicGateNet():\n","\n","    def __init__(self):\n","        def weight_init():\n","            np.random.seed(1)\n","            weights = np.random.randn(2)\n","            bias = np.random.rand(1)\n","\n","            return weights, bias\n","\n","        self.weights, self.bias = weight_init()\n","\n","    def predict(self, x):\n","        W = self.weights.reshape(-1, 1)\n","        b = self.bias\n","\n","        y_pred = sigmoid(np.dot(x, W) + b)\n","        return y_pred\n","\n","    def loss(self, x, y_true):\n","        y_pred = self.predict(x)\n","        return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","\n","    def get_gradient(self, x, t):\n","        def loss_grad(grad):\n","            return self.loss(x, t)\n","\n","        grad_W = differential(loss_grad, self.weights)\n","        grad_b = differential(loss_grad, self.bias)\n","\n","        return grad_W, grad_b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbNDoH_3zbGZ"},"source":["### AND Gate"]},{"cell_type":"markdown","metadata":{"id":"2P-ib8_RzHTh"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"rRiaACA6zGom"},"source":["AND = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([[0], [0], [0], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = AND.get_gradient(X, Y)\n","\n","  AND.weights -= lr * grad_W\n","  AND.bias -= lr * grad_b\n","\n","  loss = AND.loss(X, Y)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 0:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i, loss, AND.weights, AND.bias))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZoyQv_czT7R"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"-7CvWgc9zREa"},"source":["print(AND.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HoMXNiXWzts-"},"source":["### OR Gate"]},{"cell_type":"markdown","metadata":{"id":"DZ79pc4jzw3O"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"8gnLmAyQzuoL"},"source":["OR = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_2 = np.array([[0], [1], [1], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = OR.get_gradient(X, Y_2)\n","\n","  OR.weights -= lr * grad_W\n","  OR.bias -= lr * grad_b\n","\n","  loss = OR.loss(X, Y_2)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 0:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i, loss, OR.weights, OR.bias))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWmEtX_VnLSI"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"JwPpOs3-z2vU"},"source":["print(OR.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEBhczCIz57Q"},"source":["### NAND Gate"]},{"cell_type":"markdown","metadata":{"id":"TzQaaHKKz8sZ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"h463QUQRz8PS"},"source":["NAND = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_3 = \"\"\" 구현\"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jR-rHaTU0Mga"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"WpzKW6sm0Ghp"},"source":["print(NAND.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiTWfSQ60Zl2"},"source":["### XOR Gate"]},{"cell_type":"markdown","metadata":{"id":"hmmL0VIu0bXq"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"0CGm0r1M0a9M"},"source":["XOR = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_4 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad_W, grad_b = XOR.get_gradient(X, Y_4)\n","\n","  XOR.weights -= lr * grad_W\n","  XOR.bias -= lr * grad_b\n","\n","  loss = XOR.loss(X, Y_4)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 99:\n","    print('Epoch: {}, loss: {}, Weights: {}, Bias: {}'.format(i+1, loss, XOR.weights, XOR.bias))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cy-ktElI0o5P"},"source":["#### 테스트"]},{"cell_type":"code","source":["print(XOR.predict(X))"],"metadata":{"id":"2xHq1r8brp_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAlq_-6E1nIq"},"source":["#### 2층 신경망으로 XOR 게이트 구현(1)\n","\n","- 얕은 신경망, Shallow Neural Network\n","\n","- 두 논리게이트(NAND, OR)를 통과하고  \n","  AND 게이트로 합쳐서 구현\n","\n","- 06 신경망 구조 참고"]},{"cell_type":"code","metadata":{"id":"mr7nYMG20jTo"},"source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","\n","s1 = NAND.predict(X)\n","s2 = OR.predict(X)\n","X_2 = np.array([s1, s2]).T.reshape(-1, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkTDx8Ah1xHY"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"LK2iD5A91yWQ"},"source":["print(AND.predict(X_2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-SK4G262Agn"},"source":["#### 2층 신경망으로 XOR 게이트 구현(2)\n","- 클래스로 구현"]},{"cell_type":"code","metadata":{"id":"8RpnHCRZ1zwr"},"source":["class XORNet():\n","\n","  def __init__(self):\n","      np.random.seed(1)\n","\n","      def weight_init():\n","         params = {}\n","         params['W1'] = np.random.randn(2)\n","         params['b1'] = np.random.rand(2)\n","         params['W2'] = np.random.randn(2)\n","         params['b2'] = np.random.rand(1)\n","         return params\n","\n","      self.params = weight_init()\n","\n","  def predict(self, x):\n","      \"\"\"\n","      구현\n","      \"\"\"\n","\n","      return y\n","\n","  def loss(self, x, y_true):\n","      y_pred = self.predict(x)\n","      return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","  def get_gradient(self, x, t):\n","      def loss_grad(grad):\n","          return self.loss(x, t)\n","\n","      \"\"\"\n","      구현\n","      \"\"\"\n","\n","      return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lplK_x0l2YLh"},"source":["#### 하이퍼 파라미터(Hyper Parameter)\n","- 재조정"]},{"cell_type":"code","metadata":{"id":"qf-3wWSv2b7l"},"source":["lr = 0.3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmHKd45d2JbJ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"cQNd3XVd2Gj7"},"source":["XOR = XORNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","  grad = XOR.get_gradient(X, Y_5)\n","\n","  for key in ('W1', 'b1', 'W2', 'b2'):\n","    XOR.params[key] -= lr * grad[key]\n","\n","  loss = XOR.loss(X, Y_5)\n","  # print(loss)\n","  train_loss_list.append(loss)\n","\n","  if i % 100 == 99:\n","    print('Epoch: {}, loss: {}'.format(i+1, loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIV_GsoG2eDs"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"Dpr0nZhc2Szr"},"source":["print(XOR.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1IuDL8R7wrx"},"source":["## 다중 클래스 분류 : MNIST Dataset"]},{"cell_type":"markdown","metadata":{"id":"9CiJ5Gmq9Wpa"},"source":["### 배치 처리\n","- 학습 데이터 전체를 한번에 진행하지 않고  \n","  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n","\n","- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n","  미니 배치 학습법(mini-batch learning)이라고도 부름"]},{"cell_type":"markdown","metadata":{"id":"YUDNWwj49byH"},"source":["#### 신경망 구현 : MNIST"]},{"cell_type":"markdown","metadata":{"id":"WjBRQYlP74GM"},"source":["#### 필요한 모듈 임포트"]},{"cell_type":"code","metadata":{"id":"h0lJbkuW71lm"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from keras.datasets import mnist\n","import time\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDvtEiD77_gu"},"source":["#### 데이터 로드"]},{"cell_type":"code","metadata":{"id":"4WL7zXMl_uo9"},"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_rNg5Jn8FRA"},"source":["#### 데이터 확인"]},{"cell_type":"code","metadata":{"id":"u4wpsQGA8BOO"},"source":["print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pU7nvkHO8IFR"},"source":["img = x_train[0]\n","print(img.shape)\n","print(img.min(), img.max())\n","plt.imshow(img, cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbBA1Kl18KGT"},"source":["label = y_train[0]\n","print(label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTFu8i-z8U_C"},"source":["#### 데이터 전처리 (Data Preprocessing)"]},{"cell_type":"code","metadata":{"id":"q76pjKDVftHJ"},"source":["def flatten_for_mnist(x):\n","    temp = np.zeros((x.shape[0], x[0].size))\n","\n","    for idx, data in enumerate(x):\n","        temp[idx ,:] = data.flatten()\n","\n","    return temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvMWrDOR8Mns"},"source":["x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train = flatten_for_mnist(x_train)\n","x_test = flatten_for_mnist(x_test)\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n","y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n","\n","print(y_train_ohe.shape)\n","print(y_test_ohe.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LjpWz0dotJs"},"source":["print(x_train[0].max(), x_train[0].min())\n","print(y_train_ohe[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5GUaa92Y9RhY"},"source":["#### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","metadata":{"id":"sk3FXXLi9Th5"},"source":["epochs = 2\n","lr = 0.1\n","batch_size = 100\n","train_size = x_train.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lMJ0h8p8iZl"},"source":["#### 사용되는 함수들(Util Functions)"]},{"cell_type":"code","metadata":{"id":"bSlqZ2Xx8hFn"},"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def mean_squared_error(y_pred, y_true):\n","    return np.mean(np.power(y_true - y_pred, 2))\n","\n","def cross_entropy_error(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    return -np.sum(y_true * np.log(y_pred + delta))\n","\n","def cross_entropy_error_for_batch(y_pred, y_true):\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape(1, -1)\n","        y_pred = y_pred.reshape(1, -1)\n","    delta = 1e-7\n","    batch_size = y_pred.shape[0]\n","    return -np.sum(y_true * np.log(y_pred + delta)) / batch_size\n","\n","def cross_entropy_error_for_bin(y_pred, y_true):\n","    return 0.5 * np.sum(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","    return y\n","\n","def differential_1d(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","      temp_val = x[i]\n","\n","      x[i] = temp_val + eps\n","      f_h1 = f(x)\n","      x[i] = temp_val - eps\n","      f_h2 = f(x)\n","\n","      diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","      x[i] = temp_val\n","\n","    return diff_value\n","\n","def differential_2d(f, X):\n","    if X.ndim == 1:\n","        return differential_1d(f, X)\n","    else :\n","        grad = np.zeros_like(X)\n","\n","        for idx, x in enumerate(X):\n","            grad[idx] = differential_1d(f, x)\n","\n","        return grad\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSoV9fyj8_u7"},"source":["#### 2층 신경망으로 구현"]},{"cell_type":"code","metadata":{"id":"XBObD5Fw89HI"},"source":["class MyModel():\n","\n","  def __init__(self):\n","\n","      def weight_init(input_nodes, hidden_nodes, output_nodes):\n","         np.random.seed(777)\n","\n","\n","         params = {}\n","         params['W1'] = 0.01 * np.random.randn(input_nodes, hidden_nodes)\n","         params['b1'] = np.zeros(hidden_nodes)\n","         params['W2'] = 0.01 * np.random.randn(hidden_nodes, output_nodes)\n","         params['b2'] = np.zeros(output_nodes)\n","\n","         return params\n","\n","      self.params = weight_init(784, 64, 10)\n","\n","  def predict(self, x):\n","      W1, W2 = self.params['W1'], self.params['W2']\n","      b1, b2 = self.params['b1'], self.params['b2']\n","\n","      A1 = np.dot(x, W1) + b1\n","      Z1 = sigmoid(A1)\n","      A2 = np.dot(Z1, W2) + b2\n","      y = softmax(A2)\n","      return y\n","\n","  def loss(self, x, y_true):\n","      y_pred = self.predict(x)\n","      return cross_entropy_error_for_bin(y_pred, y_true)\n","\n","  def accuracy(self, x, y_true):\n","      y_pred = self.predict(x)\n","      y_argmax = np.argmax(y_pred, axis=1)\n","      y_true_argmax = np.argmax(y_true, axis=1)\n","\n","      accuracy = np.mean(y_argmax == y_true_argmax)\n","      return accuracy\n","\n","  def get_gradient(self, x, t):\n","      def loss_grad(grad):\n","          return self.loss(x, t)\n","\n","      grad = {}\n","      grad['W1'] = differential_2d(loss_grad, self.params['W1'])\n","      grad['b1'] = differential_2d(loss_grad, self.params['b1'])\n","      grad['W2'] = differential_2d(loss_grad, self.params['W2'])\n","      grad['b2'] = differential_2d(loss_grad, self.params['b2'])\n","\n","      return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"maKNIlK-xJ5k"},"source":["#### 모델 생성 및 학습\n","- 시간 많이 소요"]},{"cell_type":"code","metadata":{"id":"XSEARgNIop8t"},"source":["model = MyModel()\n","\n","train_loss_list = list()\n","train_acc_list = list()\n","test_acc_list = list()\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","start_time = time.time()\n","\n","for i in tqdm(range(epochs)):\n","\n","    batch_idx = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_idx]\n","    y_batch = y_train_ohe[batch_idx]\n","\n","    grad = model.get_gradient(x_batch, y_batch)\n","\n","    for key in grad.keys():\n","        model.params[key] -= lr * grad[key]\n","\n","    loss = model.loss(x_batch, y_batch)\n","    train_loss_list.append(loss)\n","\n","    train_accuracy = model.accuracy(x_train, y_train_ohe)\n","    test_accuracy = model.accuracy(x_test, y_test_ohe)\n","    train_acc_list.append(train_accuracy)\n","    test_acc_list.append(test_accuracy)\n","\n","    print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Test Accuracy: {}'.format(i+1, loss, train_accuracy, test_accuracy))\n","\n","end_time = time.time()\n","\n","print('총 학습 소요시간: {:.3f}s'.format(end_time - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7nL8f20x4zl"},"source":["### 모델의 결과\n","- 모델은 학습이 잘 될 수도, 잘 안될 수도 있음\n","\n","- 만약, 학습이 잘 되지 않았다면,  \n","  학습이 잘 되기 위해서 어떠한 조치를 취해야 하는가?\n","  - 다양한 학습관련 기술이 존재"]}]}